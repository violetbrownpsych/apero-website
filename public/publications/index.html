<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.120.4">
<title> | Violet Brown</title>


<meta property="twitter:site" content="@violetsarebrown">
<meta property="twitter:creator" content="@violetsarebrown">







  
    
  
<meta name="description" content="Visiting Assistant Professor of Psychology at Carleton College.">


<meta property="og:site_name" content="Violet Brown">
<meta property="og:title" content=" | Violet Brown">
<meta property="og:description" content="Visiting Assistant Professor of Psychology at Carleton College." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.violetabrown.com/publications/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://www.violetabrown.com/publications/sidebar-listing.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://www.violetabrown.com/publications/sidebar-listing.jpg" >
    
    
  <meta itemprop="name" content="">
<meta itemprop="description" content="Visiting Assistant Professor of Psychology at Carleton College.">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/VB_logo.PNG" type="image/x-icon">
  <link rel="icon" href="/img/VB_logo.PNG" type="image/x-icon">
  
    <link rel="alternate" type="application/rss+xml" href="https://www.violetabrown.com/publications/index.xml" title="Violet Brown" />
  
  
  <link rel="stylesheet" href="/style.main.min.fc600ce742546fe629720c5a8ad0a1143c5cccce1be48e78180e574fed054b26.css" integrity="sha256-/GAM50JUb&#43;YpcgxaitChFDxczM4b5I54GA5XT&#43;0FSyY=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.246e5a66bb6a18a599cc04627eb601304290680d3fc34f912389a3a5d8123494.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container list-sidebar">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://www.violetabrown.com" title="Home">
      <img src="/img/VB_logo.PNG" class="dib db-l h2 w-auto" alt="Violet Brown">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/publications/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/resources_tutorials" title="Resources and Tutorials">Resources and Tutorials</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="http://violetabrown.com/pdfs/brown_cv.pdf" title="CV">CV</a>
      
      
    </div>
  </nav>
</header>


<main class="page-main pa4 section" role="main">
  <section class="blog-content mw7 center">
    
      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_2025/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_2025/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_2025/" class="db">Measuring the Dual-Task Costs of Audiovisual Speech Processing Across Levels of Background Noise</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Successful communication requires that listeners not only identify speech, but do so while maintaining performance on other tasks, like remembering what a conversational partner said or paying attention while driving. This set of four experiments systematically evaluated how audiovisual speech—which reliably improves speech intelligibility—affects dual-task costs during speech perception (i.e., one facet of listening effort). Results indicated that audiovisual speech reduces dual-task costs in difficult listening conditions (those in which visual cues substantially benefit intelligibility), but may actually increase costs in easy conditions—a pattern of results that was internally replicated multiple times. This study also presents a novel dual-task paradigm specifically designed to facilitate conducting dual-task research remotely. Given the novelty of the task, this study includes psychometric experiments that establish positive and negative control, assess convergent validity, measure task sensitivity relative to a commonly-used dual-task paradigm, and generate performance curves across a range of listening conditions. Thus, in addition to evaluating the effects of audiovisual speech across a wide range of background noise levels, this study enables other researchers to address theoretical questions related to the cognitive mechanisms supporting speech processing beyond the specific issues addressed here and without being limited to in-person research.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/sewell_etal_2023/" class="db">
        
          <img src="https://www.violetabrown.com/publications/sewell_etal_2023/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/sewell_etal_2023/" class="db">The Effects of Temporal Cues, Point-Light Displays, and Faces on Speech Identification and Listening Effort</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Among the most robust findings in speech research is that the presence of a talking face improves the intelligibility of spoken language. Talking faces supplement the auditory signal by providing fine phonetic cues based on the placement of the articulators, as well as temporal cues to when speech is occurring. In this study, we varied the amount of information contained in the visual signal, ranging from temporal information alone to a natural talking face. Participants were presented with spoken sentences in energetic or informational masking in four different visual conditions: audio-only, a modulating circle providing temporal cues to salient features of the speech, a digitally rendered point-light display showing lip movement, and a natural talking face. We assessed both sentence identification accuracy and self-reported listening effort. Audiovisual benefit for intelligibility was observed for the natural face in both informational and energetic masking, but the digitally rendered point-light display only provided benefit in energetic masking. Intelligibility for speech accompanied by the modulating circle did not differ from the audio-only conditions in either masker type. Thus, the temporal cues used here were insufficient to improve speech intelligibility in noise, but some types of digital point-light displays may contain enough phonetic detail to produce modest improvements in speech identification in noise.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Katrina Sewell, Violet A. Brown, Grace Farwell, Maya Rogers, Xingyi Zhang, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/strand_brown_2023/" class="db">
        
          <img src="https://www.violetabrown.com/publications/strand_brown_2023/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/strand_brown_2023/" class="db">Spread the Word: Enhancing Replicability of Speech Research Through Stimulus Sharing</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Purpose: The ongoing replication crisis within and beyond psychology has revealed the numerous ways in which flexibility in the research process can affect study outcomes. In speech research, examples of these “researcher degrees of freedom” include the particular syllables, words, or sentences presented; the talkers who produce the stimuli and the instructions given to them; the population tested; whether and how stimuli are matched on amplitude; the type of masking noise used and its presentation level; and many others. In this research note, we argue that even seemingly minor methodological choices have the potential to affect study outcomes. To that end, we present a reanalysis of six existing data sets on spoken word identification in noise to assess how differences in talkers, stimulus processing, masking type, and listeners affect identification accuracy.
Conclusions: Our reanalysis revealed relatively low correlations among word identification rates across studies. The data suggest that some of the seemingly innocuous methodological details that differ across studies—details that cannot possibly be reported in text given the idiosyncrasies inherent to speech—introduce unknown variability that may affect replicability of our findings. We therefore argue that publicly sharing stimuli is a crucial step toward improved replicability in speech research.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Julia F. Strand &amp; Violet A. Brown</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_strand_2023/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_strand_2023/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_strand_2023/" class="db">Preregistration: Practical Considerations for Speech, Language, and Hearing Research</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">In the last decade, psychology and other sciences have implemented numerous reforms to improve the robustness of our research, many of which are based on increasing transparency throughout the research process. Among these reforms is the practice of preregistration, in which researchers create a time- stamped and uneditable document before data collection that describes the methods of the study, how the data will be analyzed, the sample size, and many other decisions. The current article highlights the benefits of preregistration with a focus on the specific issues that speech, language, and hearing researchers are likely to encounter, and additionally provides a tutorial for writing preregistrations. Conclusions: Although rates of preregistration have increased dramatically in recent years, the practice is still relatively uncommon in research on speech, language, and hearing. Low rates of adoption may be driven by a lack of under- standing of the benefits of preregistration (either generally or for our discipline in particular) or uncertainty about how to proceed if it becomes necessary to deviate from the preregistered plan. Alternatively, researchers may see the ben- efits of preregistration but not know where to start, and gathering this informa- tion from a wide variety of sources is arduous and time consuming. This tutorial addresses each of these potential roadblocks to preregistration and equips readers with tools to facilitate writing preregistrations for research on speech, language, and hearing.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/wilbiks_etal_2022/" class="db">
        
          <img src="https://www.violetabrown.com/publications/wilbiks_etal_2022/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/wilbiks_etal_2022/" class="db">Speech and Non-Speech Measures of Audiovisual Integration are not Correlated</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Many natural events generate both visual and auditory signals, and humans are remarkably adept at integrating information from those sources. However, individuals appear to differ markedly in their ability or propensity to combine what they hear with what they see. Individual differences in audiovisual integration have been established using a range of materials, including speech stimuli (seeing and hearing a talker) and simpler audiovisual stimuli (seeing flashes of light combined with tones). Although there are multiple tasks in the literature that are referred to as “measures of audiovisual integration,” the tasks themselves differ widely with respect to both the type of stimuli used (speech versus non-speech) and the nature of the tasks themselves (e.g., some tasks use conflicting auditory and visual stimuli whereas others use congruent stimuli). It is not clear whether these varied tasks are actually measuring the same underlying construct: audiovisual integration. This study tested the relationships among four commonly-used measures of audiovisual integration, two of which use speech stimuli (susceptibility to the McGurk effect and a measure of audiovisual benefit), and two of which use non-speech stimuli (the sound-induced flash illusion and audiovisual integration capacity). We replicated previous work showing large individual differences in each measure but found no significant correlations among any of the measures. These results suggest that tasks that are commonly referred to as measures of audiovisual integration may be tapping into different parts of the same process or different constructs entirely.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Jonathan M. P. Wilbiks, Violet A. Brown, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_etal_2022/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_etal_2022/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_etal_2022/" class="db">Revisiting the Target-Masker Linguistic Similarity Hypothesis</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">The linguistic similarity hypothesis states that it is more difficult to segregate target and masker speech when they are linguistically similar. For example, recognition of English target speech should be more impaired by the presence of Dutch masking speech than Mandarin masking speech because Dutch and English are more linguistically similar than Mandarin and English. Across four experiments, English target speech was consistently recognized more poorly when presented in English masking speech than in silence, speech-shaped noise, or an unintelligible masker (i.e., Dutch or Mandarin). However, we found no evidence for graded masking effects—Dutch did not impair performance more than Mandarin in any experiment, despite 650 participants being tested. This general pattern was consistent when using both a cross-modal paradigm (in which target speech was lipread and maskers were presented aurally; Experiments 1a and 1b) and an auditory-only paradigm (in which both the targets and maskers were presented aurally; Experiments 2a and 2b). These findings suggest that the linguistic similarity hypothesis should be refined to reflect the existing evidence: There is greater release from masking when the masker language differs from the target speech than when it is the same as the target speech. However, evidence that unintelligible maskers impair speech identification to a greater extent when they are more linguistically similar to the target language remains elusive.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Naseem H. Dillman-Hasso, ZhaoBin Li, Lucia Ray, Ellen Mamantov, Kristin J. Van Engen, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/mclaughlin_etal_2022/" class="db">
        
          <img src="https://www.violetabrown.com/publications/mclaughlin_etal_2022/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/mclaughlin_etal_2022/" class="db">Revisiting the Relationship Between Implicit Racial Bias and Audiovisual Benefit for Nonnative-Accented Speech</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Speech intelligibility is improved when the listener can see the talker in addition to hearing their voice. Notably, though,  previous work has suggested that this “audiovisual benefit” for nonnative (i.e., foreign-accented) speech is smaller than the  benefit for native speech, an effect that may be partially accounted for by listeners’ implicit racial biases (Yi et al., 2013, The  Journal of the Acoustical Society of America, 134[5], EL387–EL393.). In the present study, we sought to replicate these find- ings in a significantly larger sample of online participants. In a direct replication of Yi et al. (Experiment 1), we found that  audiovisual benefit was indeed smaller for nonnative-accented relative to native-accented speech. However, our results did  not support the conclusion that implicit racial biases, as measured with two types of implicit association tasks, were related  to these differences in audiovisual benefit for native and nonnative speech. In a second experiment, we addressed a potential  confound in the experimental design; to ensure that the difference in audiovisual benefit was caused by a difference in accent  rather than a difference in overall intelligibility, we reversed the overall difficulty of each accent condition by presenting them  at different signal-to-noise ratios. Even when native speech was presented at a much more difficult intelligibility level than  nonnative speech, audiovisual benefit for nonnative speech remained poorer. In light of these findings, we discuss alternative  explanations of reduced audiovisual benefit for nonnative speech, as well as methodological considerations for future work  examining the intersection of social, cognitive, and linguistic processes.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Drew J. McLaughlin, Violet A. Brown, Sita Carraturo, &amp; Kristin J. Van Engen</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_vanengen_peelle_2021/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_vanengen_peelle_2021/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_vanengen_peelle_2021/" class="db">Face Mask Type Affects Audiovisual Speech Intelligibility and Subjective Listening Effort in Young and Older Adults</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Identifying speech requires that listeners make rapid use of fine-grained acoustic cues—a process that is facilitated by being able to see the talker’s face. Face masks present a challenge to this process because they can both alter acoustic information and conceal the talker’s mouth. Here, we investigated the degree to which different types of face masks and noise levels affect speech intelligibility and subjective listening effort for young (N = 180) and older (N = 180) adult listeners. We found that in quiet, mask type had little influence on speech intelligibility relative to speech produced without a mask for both young and older adults. However, with the addition of moderate (− 5 dB SNR) and high (− 9 dB SNR) levels of background noise, intelligibility dropped substantially for all types of face masks in both age groups. Across noise levels, transparent face masks and cloth face masks with filters impaired performance the most, and surgical face masks had the smallest influence on intelligibility. Participants also rated speech produced with a face mask as more effortful than unmasked speech, particularly in background noise. Although young and older adults were similarly affected by face masks and noise in terms of intelligibility and subjective listening effort, older adults showed poorer intelligibility overall and rated the speech as more effortful to process relative to young adults. This research will help individuals make more informed decisions about which types of masks to wear in various communicative settings.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Kristin J. Van Engen, &amp; Jonathan E. Peelle</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_fox_strand_2021/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_fox_strand_2021/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_fox_strand_2021/" class="db">&ldquo;Where Are the . . .Fixations?:&rdquo; Grammatical Number Cues Guide Anticipatory Fixations to Upcoming Referents and Reduce Lexical Competition</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Listeners make use of contextual cues during continuous speech processing that help overcome the limitations of the acoustic input. These semantic, grammatical, and pragmatic cues facilitate prediction of upcoming words and/or reduce the lexical search space by inhibiting activation of contextually inappropriate words that share phonological information with the target. The current study used the visual world paradigm to assess whether and how listeners use contextual cues about grammatical number during sentence processing by presenting target words in carrier phrases that were grammatically unconstraining (&ldquo;Click on the . . .&rdquo;) or grammatically constraining (&ldquo;Where is/are the . . .&rdquo;). Prior to the onset of the target word, listeners were already more likely to fixate on plural objects in the &ldquo;Where are the . . .&rdquo; context than the &ldquo;Where is the . . .&rdquo; context, indicating that they used the construction of the verb to anticipate the referent. Further, participants showed less interference from cohort competitors when the sentence frame made them contextually inappropriate, but still fixated on those words more than on phonologically unrelated distractor words. These results suggest that listeners rapidly and flexibly make use of contextual cues about grammatical number while maintaining sensitivity to the bottom-up input.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Neal P. Fox, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_2021/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_2021/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_2021/" class="db">An Introduction to Linear Mixed-Effects Modeling in R</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at 
<a href="https://osf.io/v6qag/" target="_blank" rel="noopener">https://osf.io/v6qag/</a>, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
    



<div class="list-pagination w-100">
  <ul class="paginate list pl0 tc dt w-100 ma0">
    <li class="dtc w-25 tl v-mid">
    
      <span class="previous pa2 db fw6">&larr; Newer</span>
    
    </li>
    <li class="dtc w-50 v-mid">
      <span class="page_number fw6 f7 ttu tracked">1 of 3</span>
    </li>
    <li class="dtc w-25 tr v-mid">
    
      <a href="/publications/page/2/" class="next pa2 db fw6" aria-label="Next">Older &rarr;</a>
    
    </li>
  </ul>
</div>


  </section>
</main>
<aside class="page-sidebar" role="complementary">
                         
 


                       
 











  <img src="/publications/sidebar-listing.jpg" class="db ma0">


<div class="blog-info ph4 pt4 pb4 pb0-l">
  

  <h1 class="f3">Academic Publications (by date)</h1>
  <p class="f6 lh-copy measure">Here&rsquo;s a list of my publications along with links to the publications and PDFs.</p>
  <p class="f7 measure lh-copy i mh0-l">Written by Violet Brown</p>


  
  <div class="dib bt bw1 b--black-10">
    
      <small class="db f7 pt3"><a href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" class="dib fw7 ttu">Google Scholar</a></small>
    
    
    
    
      <small class="db f7 pt3"><a href="/tags/" class="dib fw7 ttu">Tags</a></small>
    
  </div>


</div>


</aside>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2026 Violet A. Brown
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/profile/violetsarebrown.bsky.social" title="cloud-sun" target="_blank" rel="me noopener">
      <i class="fas fa-cloud-sun fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/violetsarebrown?lang=en" title="twitter" target="_blank" rel="me noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://osf.io/fzqc8/" title="osf" target="_blank" rel="me noopener">
      <i class="ai ai-osf fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/violetbrownpsych/apero-website" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0001-5310-6499" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="ai ai-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:violet@violetabrown.com" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
