<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Violet Brown</title>
    <link>https://www.violetabrown.com/publications/</link>
    <description>Recent content on Violet Brown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 29 Nov 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.violetabrown.com/publications/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Effects of Temporal Cues, Point-Light Displays, and Faces on Speech Identification and Listening Effort</title>
      <link>https://www.violetabrown.com/publications/sewell_etal_2023/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/sewell_etal_2023/</guid>
      <description>Among the most robust findings in speech research is that the presence of a talking face improves the intelligibility of spoken language. Talking faces supplement the auditory signal by providing fine phonetic cues based on the placement of the articulators, as well as temporal cues to when speech is occurring. In this study, we varied the amount of information contained in the visual signal, ranging from temporal information alone to a natural talking face.</description>
    </item>
    <item>
      <title>Preregistration: Practical Considerations for Speech, Language, and Hearing Research</title>
      <link>https://www.violetabrown.com/publications/brown_strand_2023/</link>
      <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_strand_2023/</guid>
      <description>In the last decade, psychology and other sciences have implemented numerous reforms to improve the robustness of our research, many of which are based on increasing transparency throughout the research process. Among these reforms is the practice of preregistration, in which researchers create a time- stamped and uneditable document before data collection that describes the methods of the study, how the data will be analyzed, the sample size, and many other decisions.</description>
    </item>
    <item>
      <title>Spread the Word: Enhancing Replicability of Speech Research Through Stimulus Sharing</title>
      <link>https://www.violetabrown.com/publications/strand_brown_2023/</link>
      <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_brown_2023/</guid>
      <description>Purpose: The ongoing replication crisis within and beyond psychology has revealed the numerous ways in which flexibility in the research process can affect study outcomes. In speech research, examples of these “researcher degrees of freedom” include the particular syllables, words, or sentences presented; the talkers who produce the stimuli and the instructions given to them; the population tested; whether and how stimuli are matched on amplitude; the type of masking noise used and its presentation level; and many others.</description>
    </item>
    <item>
      <title>Speech and Non-Speech Measures of Audiovisual Integration are not Correlated</title>
      <link>https://www.violetabrown.com/publications/wilbiks_etal_2022/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/wilbiks_etal_2022/</guid>
      <description>Many natural events generate both visual and auditory signals, and humans are remarkably adept at integrating information from those sources. However, individuals appear to differ markedly in their ability or propensity to combine what they hear with what they see. Individual differences in audiovisual integration have been established using a range of materials, including speech stimuli (seeing and hearing a talker) and simpler audiovisual stimuli (seeing flashes of light combined with tones).</description>
    </item>
    <item>
      <title>Revisiting the Target-Masker Linguistic Similarity Hypothesis</title>
      <link>https://www.violetabrown.com/publications/brown_etal_2022/</link>
      <pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_etal_2022/</guid>
      <description>Purpose: The linguistic similarity hypothesis states that it is more difficult to segregate target and masker speech when they are linguistically similar. For example, recognition of English target speech should be more impaired by the presence of Dutch masking speech than Mandarin masking speech because Dutch and English are more linguistically similar than Mandarin and English. Across four experiments, English target speech was consistently recognized more poorly when presented in English masking speech than in silence, speech-shaped noise, or an unintelligible masker (i.</description>
    </item>
    <item>
      <title>Revisiting the Relationship Between Implicit Racial Bias and Audiovisual Benefit for Nonnative-Accented Speech</title>
      <link>https://www.violetabrown.com/publications/mclaughlin_etal_2022/</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/mclaughlin_etal_2022/</guid>
      <description>Speech intelligibility is improved when the listener can see the talker in addition to hearing their voice. Notably, though, previous work has suggested that this “audiovisual benefit” for nonnative (i.e., foreign-accented) speech is smaller than the benefit for native speech, an effect that may be partially accounted for by listeners’ implicit racial biases (Yi et al., 2013, The Journal of the Acoustical Society of America, 134[5], EL387–EL393.). In the present study, we sought to replicate these find- ings in a significantly larger sample of online participants.</description>
    </item>
    <item>
      <title>Face Mask Type Affects Audiovisual Speech Intelligibility and Subjective Listening Effort in Young and Older Adults</title>
      <link>https://www.violetabrown.com/publications/brown_vanengen_peelle_2021/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_vanengen_peelle_2021/</guid>
      <description>Identifying speech requires that listeners make rapid use of fine-grained acoustic cues—a process that is facilitated by being able to see the talker’s face. Face masks present a challenge to this process because they can both alter acoustic information and conceal the talker’s mouth. Here, we investigated the degree to which different types of face masks and noise levels affect speech intelligibility and subjective listening effort for young (N = 180) and older (N = 180) adult listeners.</description>
    </item>
    <item>
      <title>&#34;Where Are the . . .Fixations?:&#34; Grammatical Number Cues Guide Anticipatory Fixations to Upcoming Referents and Reduce Lexical Competition</title>
      <link>https://www.violetabrown.com/publications/brown_fox_strand_2021/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_fox_strand_2021/</guid>
      <description>Listeners make use of contextual cues during continuous speech processing that help overcome the limitations of the acoustic input. These semantic, grammatical, and pragmatic cues facilitate prediction of upcoming words and/or reduce the lexical search space by inhibiting activation of contextually inappropriate words that share phonological information with the target. The current study used the visual world paradigm to assess whether and how listeners use contextual cues about grammatical number during sentence processing by presenting target words in carrier phrases that were grammatically unconstraining (&amp;ldquo;Click on the .</description>
    </item>
    <item>
      <title>An Introduction to Linear Mixed-Effects Modeling in R</title>
      <link>https://www.violetabrown.com/publications/brown_2021/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_2021/</guid>
      <description>This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers.</description>
    </item>
    <item>
      <title>Understanding Speech amid the Jingle and Jangle: Recommendations for Improving Measurement Practices in Listening Effort Research</title>
      <link>https://www.violetabrown.com/publications/strand_etal_2021/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_etal_2021/</guid>
      <description>The latent constructs psychologists study are typically not directly accessible, so researchers must design measurement instruments that are intended to provide insights about those constructs. Construct validation—assessing whether instruments measure what they intend to—is therefore critical for ensuring that the conclusions we draw actually reflect the intended phenomena. Insufficient construct validation can lead to the jingle fallacy—falsely assuming two instruments measure the same construct because the instruments share a name—and the jangle fallacy—falsely assuming two instruments measure different constructs because the instruments have different names.</description>
    </item>
    <item>
      <title>Recall of Speech is Impaired by Subsequent Masking Noise: A Replication of Rabbitt (1968) Experiment 2</title>
      <link>https://www.violetabrown.com/publications/guang_etal_2021/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/guang_etal_2021/</guid>
      <description>The presence of masking noise can impair speech intelligibility and increase the cognitive resources necessary to understand speech. The first study to demonstrate the negative cognitive consequences of noisy speech—published by Rabbitt in 1968—found that participants had poorer recall for aurally presented digits early in a list when later digits were presented in noise relative to quiet. However, despite being cited nearly 500 times and providing the foundation for a wealth of subsequent research on the topic, the original study has never been directly replicated.</description>
    </item>
    <item>
      <title>Rapid Adaptation to Fully Intelligible Nonnative-Accented Speech Reduces Listening Effort</title>
      <link>https://www.violetabrown.com/publications/brown_etal_2020/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_etal_2020/</guid>
      <description>In noisy settings or when listening to an unfamiliar talker or accent, it can be difficult to understand spoken language. This difficulty typically results in reductions in speech intelligibility, but may also increase the effort necessary to process the speech even when intelligibility is unaffected. In this study, we used a dual-task paradigm and pupillometry to assess the cognitive costs associated with processing fully intelligible accented speech, predicting that rapid perceptual adaptation to an accent would result in decreased listening effort over time.</description>
    </item>
    <item>
      <title>Talking Points: A Modulating Circle Increases Listening Effort Without Improving Speech Recognition in Young Adults</title>
      <link>https://www.violetabrown.com/publications/strand_brown_barbour_2020/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_brown_barbour_2020/</guid>
      <description>Speech recognition is improved when the acoustic input is accompanied by visual cues provided by a talking face (Erber in Journal of Speech and Hearing Research, 12(2), 423–425, 1969; Sumby &amp;amp; Pollack in The Journal of the Acoustical Society of America, 26(2), 212–215, 1954). One way that the visual signal facilitates speech recognition is by providing the listener with information about fine phonetic detail that complements information from the auditory signal.</description>
    </item>
    <item>
      <title>About Face: Seeing the Talker Improves Spoken  Word Recognition but Increases Listening Effort</title>
      <link>https://www.violetabrown.com/publications/brown_strand_2019/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_strand_2019/</guid>
      <description>It is widely accepted that seeing a talker improves a listener’s ability to understand what a talker is saying in background noise (e.g., Erber, 1969; Sumby &amp;amp; Pollack, 1954). The literature is mixed, however, regarding the influence of the visual modality on the listening effort required to recognize speech (e.g., Fraser, Gagné, Alepins, &amp;amp; Dubois, 2010; Sommers &amp;amp; Phelps, 2016). Here, we present data showing that even when the visual modality robustly benefits recognition, processing audiovisual speech can still result in greater cognitive load than processing speech in the auditory modality alone.</description>
    </item>
    <item>
      <title>&#34;Paying&#34; Attention to Audiovisual Speech: Do Incongruent Stimuli Incur Greater Costs?</title>
      <link>https://www.violetabrown.com/publications/brown_strand_2019_app/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_strand_2019_app/</guid>
      <description>The McGurk effect is a multisensory phenomenon in which discrepant auditory and visual speech signals typically result in an illusory percept. McGurk stimuli are often used in studies assessing the attentional requirements of audiovisual integration, but no study has directly compared the costs associated with integrating congruent versus incongruent audiovisual speech. Some evidence suggests that the McGurk effect may not be representative of naturalistic audiovisual speech processing – susceptibility to the McGurk effect is not associated with the ability to derive benefit from the addition of the visual signal, and distinct cortical regions are recruited when processing congruent versus incongruent speech.</description>
    </item>
    <item>
      <title>Publishing Open, Reproducible Research With Undergraduates</title>
      <link>https://www.violetabrown.com/publications/strand_brown_2019/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_brown_2019/</guid>
      <description>In response to growing concern in psychology and other sciences about low rates of replicability of published findings (Open Science Collaboration, 2015), there has been a movement toward conducting open and transparent research (see Chambers, 2017). This has led to changes in statistical reporting guidelines in journals (Appelbaum et al., 2018), new professional societies (e.g., Society for the Improvement of Psychological Science), frameworks for posting materials, data, code, and manuscripts (e.</description>
    </item>
    <item>
      <title>Noise Increases Listening Effort in Normal-Hearing Young Adults, Regardless of Working Memory Capacity</title>
      <link>https://www.violetabrown.com/publications/brown_strand_2018/</link>
      <pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_strand_2018/</guid>
      <description>As listening conditions worsen (e.g. background noise increases), additional cognitive effort is required to process speech. The existing literature is mixed on whether and how cognitive traits like working memory capacity moderate the amount of effort that listeners must expend to successfully understand speech. Here, we validate a dual-task measure of listening effort (Experiment 1) and demonstrate that for normal-hearing young adults, effort increases as steady-state masking noise increases, but working memory capacity is unrelated to the amount of effort expended (Experiment 2).</description>
    </item>
    <item>
      <title>Node Ordering for Rescalable Network Summarization (or, the Apparent Magic of Word Frequency and Age of Acquisition in the Lexicon)</title>
      <link>https://www.violetabrown.com/publications/brown_chen_etal_2018/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_chen_etal_2018/</guid>
      <description>How can we “scale down” an n-node network G to a smaller network, with k &amp;laquo; n nodes, so that G (approximately) maintains the important structural properties of G? There is a voluminous literature on many versions of this problem if k is given in advance, but one’s tolerance for approximation (and the resulting value of k) will vary. Here, then, we formulate a “rescalable” version of this approximation task for complex networks.</description>
    </item>
    <item>
      <title>What Accounts for Individual Differences in Susceptibility to the McGurk Effect?</title>
      <link>https://www.violetabrown.com/publications/brown_etal_2018/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_etal_2018/</guid>
      <description>The McGurk effect is a classic audiovisual speech illusion in which discrepant auditory and visual syllables can lead to a fused percept (e.g., an auditory /bɑ/ paired with a visual /gɑ/ often leads to the perception of /dɑ/). The McGurk effect is robust and easily replicated in pooled group data, but there is tremendous variability in the extent to which individual participants are susceptible to it. In some studies, the rate at which individuals report fusion responses ranges from 0% to 100%.</description>
    </item>
    <item>
      <title>Measuring Listening Effort: Convergent Validity, Sensitivity, and Links With Cognitive and Personality Measures</title>
      <link>https://www.violetabrown.com/publications/strand_etal_2018/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_etal_2018/</guid>
      <description>Purpose: Listening effort (LE) describes the attentional or cognitive requirements for successful listening. Despite substantial theoretical and clinical interest in LE, inconsistent operationalization makes it difficult to make generalizations across studies. The aims of this large-scale validation study were to evaluate the convergent validity and sensitivity of commonly used measures of LE and assess how scores on those tasks relate to cognitive and personality variables.
Method: Young adults with normal hearing (N = 111) completed 7 tasks designed to measure LE, 5 tests of cognitive ability, and 2 personality measures.</description>
    </item>
    <item>
      <title>Keep Listening: Grammatical Context Reduces But Does Not Eliminate Activation of Unexpected Words</title>
      <link>https://www.violetabrown.com/publications/strand_etal_2017/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/strand_etal_2017/</guid>
      <description>To understand spoken language, listeners combine acoustic-phonetic input with expectations derived from context (Dahan &amp;amp; Magnuson, 2006). Eye-tracking studies on semantic context have demonstrated that the activation levels of competing lexical candidates depend on the relative strengths of the bottom-up input and top-down expectations (cf. Dahan &amp;amp; Tanenhaus, 2004). In the grammatical realm, however, graded effects of context on lexical competition have been predicted (Magnuson, Tanenhaus, &amp;amp; Aslin, 2008), but not demonstrated.</description>
    </item>
  </channel>
</rss>
