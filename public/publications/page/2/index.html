<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.120.4">
<title> | Violet Brown</title>


<meta property="twitter:site" content="@violetsarebrown">
<meta property="twitter:creator" content="@violetsarebrown">







  
    
  
<meta name="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">


<meta property="og:site_name" content="Violet Brown">
<meta property="og:title" content=" | Violet Brown">
<meta property="og:description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.violetabrown.com/publications/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://www.violetabrown.com/publications/sidebar-listing.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://www.violetabrown.com/publications/sidebar-listing.jpg" >
    
    
  <meta itemprop="name" content="">
<meta itemprop="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/VB_logo.PNG" type="image/x-icon">
  <link rel="icon" href="/img/VB_logo.PNG" type="image/x-icon">
  
    <link rel="alternate" type="application/rss+xml" href="https://www.violetabrown.com/publications/index.xml" title="Violet Brown" />
  
  
  <link rel="stylesheet" href="/style.main.min.fc600ce742546fe629720c5a8ad0a1143c5cccce1be48e78180e574fed054b26.css" integrity="sha256-/GAM50JUb&#43;YpcgxaitChFDxczM4b5I54GA5XT&#43;0FSyY=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.246e5a66bb6a18a599cc04627eb601304290680d3fc34f912389a3a5d8123494.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container list-sidebar">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://www.violetabrown.com" title="Home">
      <img src="/img/VB_logo.PNG" class="dib db-l h2 w-auto" alt="Violet Brown">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/publications/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/pdfs/brown_cv.pdf" title="CV">CV</a>
      
      
    </div>
  </nav>
</header>


<main class="page-main pa4 section" role="main">
  <section class="blog-content mw7 center">
    
      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/guang_etal_2021/" class="db">
        
          <img src="https://www.violetabrown.com/publications/guang_etal_2021/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/guang_etal_2021/" class="db">Recall of Speech is Impaired by Subsequent Masking Noise: A Replication of Rabbitt (1968) Experiment 2</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">The presence of masking noise can impair speech intelligibility and increase the cognitive resources necessary to understand speech. The first study to demonstrate the negative cognitive consequences of noisy speech—published by Rabbitt in 1968—found that participants had poorer recall for aurally presented digits early in a list when later digits were presented in noise relative to quiet. However, despite being cited nearly 500 times and providing the foundation for a wealth of subsequent research on the topic, the original study has never been directly replicated. Here we report a replication attempt of that study with a large online sample and tested the robustness of the results to a variety of scoring and analytical techniques. We replicated the key finding that listening to speech in noise impairs recall for items that came earlier in the list. The results were consistent when we used the original analytical technique (an ANOVA) and a more powerful analytical technique (generalized linear mixed effects models) that was not available when the original paper was published. These findings support the claim that effortful listening can interfere with encoding or rehearsal of previously presented information.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Claire Guang, Emmett Lefkowitz, Naseem Dillman-Hasso, Violet A. Brown, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_etal_2020/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_etal_2020/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_etal_2020/" class="db">Rapid Adaptation to Fully Intelligible Nonnative-Accented Speech Reduces Listening Effort</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">In noisy settings or when listening to an unfamiliar talker or accent, it can be difficult to understand spoken language. This difficulty typically results in reductions in speech intelligibility, but may also increase the effort necessary to process the speech even when intelligibility is unaffected. In this study, we used a dual-task paradigm and pupillometry to assess the cognitive costs associated with processing fully intelligible accented speech, predicting that rapid perceptual adaptation to an accent would result in decreased listening effort over time. The behavioural and physiological paradigms provided converging evidence that listeners expend greater effort when processing nonnative- relative to native-accented speech, and both experiments also revealed an overall reduction in listening effort over the course of the experiment. Only the pupillometry experiment, however, revealed greater adaptation to nonnative- relative to native-accented speech. An exploratory analysis of the dual-task data that attempted to minimise practice effects revealed weak evidence for greater adaptation to the nonnative accent. These results suggest that even when speech is fully intelligible, resolving deviations between the acoustic input and stored lexical representations incurs a processing cost, and adaptation may attenuate this cost.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Drew, J. McLaughlin, Julia F. Strand, &amp; Kristin J. Van Engen</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/strand_brown_barbour_2020/" class="db">
        
          <img src="https://www.violetabrown.com/publications/strand_brown_barbour_2020/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/strand_brown_barbour_2020/" class="db">Talking Points: A Modulating Circle Increases Listening Effort Without Improving Speech Recognition in Young Adults</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Speech recognition is improved when the acoustic input is accompanied by visual cues provided by a talking face (Erber in Journal of Speech and Hearing Research, 12(2), 423–425, 1969; Sumby &amp; Pollack in The Journal of the Acoustical Society of America, 26(2), 212–215, 1954). One way that the visual signal facilitates speech recognition is by providing the listener with information about fine phonetic detail that complements information from the auditory signal. However, given that degraded face stimuli can still improve speech recognition accuracy (Munhall, Kroos, Jozan, &amp; Vatikiotis-Bateson in Perception &amp; Psychophysics, 66(4), 574–583, 2004), and static or moving shapes can improve speech detection accuracy (Bernstein, Auer, &amp; Takayanagi in Speech Communication, 44(1–4), 5–18, 2004), aspects of the visual signal other than fine phonetic detail may also contribute to the perception of speech. In two experiments, we show that a modulating circle providing information about the onset, offset, and acoustic amplitude envelope of the speech does not improve recognition of spoken sentences (Experiment 1) or words (Experiment 2). Further, contrary to our hypothesis, the modulating circle increased listening effort despite subjective reports that it made the word recognition task seem easier to complete (Experiment 2). These results suggest that audiovisual speech processing, even when the visual stimulus only conveys temporal information about the acoustic signal, may be a cognitively demanding process.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Julia F. Strand, Violet A. Brown, &amp; Dennis L. Barbour</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_strand_2019/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_strand_2019/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_strand_2019/" class="db">About Face: Seeing the Talker Improves Spoken  Word Recognition but Increases Listening Effort</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">It is widely accepted that seeing a talker improves a listener’s ability to understand what a talker is saying in background noise (e.g., Erber, 1969; Sumby &amp; Pollack, 1954). The literature is mixed, however, regarding the influence of the visual modality on the listening effort required to recognize speech (e.g., Fraser, Gagné, Alepins, &amp; Dubois, 2010; Sommers &amp; Phelps, 2016). Here, we present data showing that even when the visual modality robustly benefits recognition, processing audiovisual speech can still result in greater cognitive load than processing speech in the auditory modality alone. We show using a dual-task paradigm that the costs associated with audiovisual speech processing are more pronounced in easy listening conditions, in which speech can be recognized at high rates in the auditory modality alone—indeed, effort did not differ between audiovisual and audio-only conditions when the background noise was presented at a more difficult level. Further, we show that though these effects replicate with different stimuli and participants, they do not emerge when effort is assessed with a recall paradigm rather than a dual-task paradigm. Together, these results suggest that the widely cited audiovisual recognition benefit may come at a cost under more favorable listening conditions, and add to the growing body of research suggesting that various measures of effort may not be tapping into the same underlying construct (Strand et al., 2018).</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_strand_2019_app/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_strand_2019_app/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_strand_2019_app/" class="db">&ldquo;Paying&rdquo; Attention to Audiovisual Speech: Do Incongruent Stimuli Incur Greater Costs?</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">The McGurk effect is a multisensory phenomenon in which discrepant auditory and visual speech signals typically result in an illusory percept. McGurk stimuli are often used in studies assessing the attentional requirements of audiovisual integration, but no study has directly compared the costs associated with integrating congruent versus incongruent audiovisual speech. Some evidence suggests that the McGurk effect may not be representative of naturalistic audiovisual speech processing – susceptibility to the McGurk effect is not associated with the ability to derive benefit from the addition of the visual signal, and distinct cortical regions are recruited when processing congruent versus incongruent speech. In two experiments, one using response times to identify congruent and incongruent syllables and one using a dual-task paradigm, we assessed whether congruent and incongruent audiovisual speech incur different attentional costs. We demonstrated that response times to both the speech task (Experiment 1) and a secondary vibrotactile task (Experiment 2) were indistinguishable for congruent compared to incongruent syllables, but McGurk fusions were responded to more quickly than McGurk non-fusions. These results suggest that despite documented differences in how congruent and incongruent stimuli are processed, they do not appear to differ in terms of processing time or effort, at least in the open-set task speech task used here. However, responses that result in McGurk fusions are processed more quickly than those that result in non-fusions, though attentional cost is comparable for the two response types.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/strand_brown_2019/" class="db">
        
          <img src="https://www.violetabrown.com/publications/strand_brown_2019/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/strand_brown_2019/" class="db">Publishing Open, Reproducible Research With Undergraduates</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">In response to growing concern in psychology and other sciences about low rates of replicability of published findings (Open Science Collaboration, 2015), there has been a movement toward conducting open and transparent research (see Chambers, 2017). This has led to changes in statistical reporting guidelines in journals (Appelbaum et al., 2018), new professional societies (e.g., Society for the Improvement of Psychological Science), frameworks for posting materials, data, code, and manuscripts (e.g., Open Science Framework, PsyArXiv), initiatives for sharing data and collaborating (e.g., Psych Science Accelerator, Study Swap), and educational resources for teaching through replication (e.g., Collaborative Replications and Education Project). This “credibility revolution” (Vazire, 2018) provides many opportunities for researchers. However, given the recency of the changes and the rapid pace of advancements (see Houtkoop et al., 2018), it may be overwhelming for faculty to know whether and how to begin incorporating open science practices into research with undergraduates.
In this paper, we will not attempt to catalog the entirety of the open science movement (see recommended resources below for more information), but will instead highlight why adopting open science practices may be particularly beneficial to conducting and publishing research with undergraduates. The first author is a faculty member at Carleton College (a small, undergraduate-only liberal arts college) and the second is a former undergraduate research assistant (URA) and lab manager in Dr. Strand&rsquo;s lab, now pursuing a PhD at Washington University in St. Louis. We argue that open science practices have tremendous benefits for undergraduate students, both in creating publishable results and in preparing students to be critical consumers of science.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Julia F. Strand &amp; Violet A. Brown</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_strand_2018/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_strand_2018/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_strand_2018/" class="db">Noise Increases Listening Effort in Normal-Hearing Young Adults, Regardless of Working Memory Capacity</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">As listening conditions worsen (e.g. background noise increases), additional cognitive effort is required to process speech. The existing literature is mixed on whether and how cognitive traits like working memory capacity moderate the amount of effort that listeners must expend to successfully understand speech. Here, we validate a dual-task measure of listening effort (Experiment 1) and demonstrate that for normal-hearing young adults, effort increases as steady-state masking noise increases, but working memory capacity is unrelated to the amount of effort expended (Experiment 2). We propose that previous research may have overestimated the relationship between listening effort and working memory capacity by measuring listening effort using recall-based tasks. The present results suggest caution in making the general assumption that working memory capacity is related to the amount of effort expended during a listening task.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_chen_etal_2018/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_chen_etal_2018/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_chen_etal_2018/" class="db">Node Ordering for Rescalable Network Summarization (or, the Apparent Magic of Word Frequency and Age of Acquisition in the Lexicon)</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">How can we “scale down” an n-node network G to a smaller network, with k &laquo; n nodes, so that G (approximately) maintains the important structural properties of G? There is a voluminous literature on many versions of this problem if k is given in advance, but one’s tolerance for approximation (and the resulting value of k) will vary. Here, then, we formulate a “rescalable” version of this approximation task for complex networks. Specifically, we propose a node ordering version of graph summarization: permute the nodes of G so that the subgraph induced by the first k nodes is a good size-k approximation of G, averaged over the full range of possible sizes k. We consider as a case study the phonological network of English words, and discover two natural word orders (word frequency and age of acquisition) that do a surprisingly good job of rescalably summarizing the lexicon.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Xi Chen, Maryam Hedayati, Camden Sikes, Julia F. Strand, Tegan Wilson, &amp; David Liben-Nowell </p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/brown_etal_2018/" class="db">
        
          <img src="https://www.violetabrown.com/publications/brown_etal_2018/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/brown_etal_2018/" class="db">What Accounts for Individual Differences in Susceptibility to the McGurk Effect?</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">The McGurk effect is a classic audiovisual speech illusion in which discrepant auditory and visual syllables can lead to a fused percept (e.g., an auditory /bɑ/ paired with a visual /gɑ/ often leads to the perception of /dɑ/). The McGurk effect is robust and easily replicated in pooled group data, but there is tremendous variability in the extent to which individual participants are susceptible to it. In some studies, the rate at which individuals report fusion responses ranges from 0% to 100%. Despite its widespread use in the audiovisual speech perception literature, the roots of the wide variability in McGurk susceptibility are largely unknown. This study evaluated whether several perceptual and cognitive traits are related to McGurk susceptibility through correlational analyses and mixed effects modeling. We found that an individual’s susceptibility to the McGurk effect was related to their ability to extract place of articulation information from the visual signal (i.e., a more fine-grained anal- ysis of lipreading ability), but not to scores on tasks measuring attentional control, processing speed, working memory capacity, or auditory perceptual gradiency. These results provide support for the claim that a small amount of the variability in susceptibility to the McGurk effect is attributable to lipreading skill. In contrast, cognitive and perceptual abilities that are commonly used predictors in individual differences studies do not appear to underlie susceptibility to the McGurk effect.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Violet A. Brown, Maryam Hedayati, Annie Zanger, Sasha Mayn, Lucia Ray, Naseem Dillman-Hasso, &amp; Julia F. Strand</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
        
                                   
     
     
<article class="mv4 mv5-l bb">

  
  
  
  <div class="flex flex-column mb4 mb5-l  flex-row-reverse-ns">
    <figure class="ml4-ns ma0 mb2 mb0-l w-100 w-40-ns">
      <a href="/publications/strand_etal_2018/" class="db">
        
          <img src="https://www.violetabrown.com/publications/strand_etal_2018/featured_pdf.png" class="db ma0 dim" alt="">
        
      </a>
    </figure>
    <div class="flex flex-column w-100 w-60-ns pl3-ns">
      <header class="w-100">
        <h1 class="f3 mv2 lh-title underline"><a href="/publications/strand_etal_2018/" class="db">Measuring Listening Effort: Convergent Validity, Sensitivity, and Links With Cognitive and Personality Measures</a></h1>
        
      </header>
      
      <p class="f6 lh-copy mv2 flex-auto">Purpose: Listening effort (LE) describes the attentional or cognitive requirements for successful listening. Despite substantial theoretical and clinical interest in LE, inconsistent operationalization makes it difficult to make generalizations across studies. The aims of this large-scale validation study were to evaluate the convergent validity and sensitivity of commonly used measures of LE and assess how scores on those tasks relate to cognitive and personality variables.
Method: Young adults with normal hearing (N = 111) completed 7 tasks designed to measure LE, 5 tests of cognitive ability, and 2 personality measures.
Results: Scores on some behavioral LE tasks were moderately intercorrelated but were generally not correlated with subjective and physiological measures of LE, suggesting that these tasks may not be tapping into the same underlying construct. LE measures differed in their sensitivity to changes in signal-to-noise ratio and the extent to which they correlated with cognitive and personality variables.
Conclusions: Given that LE measures do not show consistent, strong intercorrelations and differ in their relationships with cognitive and personality predictors, these findings suggest caution in generalizing across studies that use different measures of LE. The results also indicate that people with greater cognitive ability appear to use their resources more efficiently, thereby diminishing the detrimental effects associated with increased background noise during language processing.</p>
      <footer class="w-100">
      
      <p class="f7 db mv1">By Julia F. Strand, Violet A. Brown, Madeleine B. Merchant, Hunter E. Brown, and Julia Smith</p>
      
      
    </footer>
    </div>
  </div>

  
  
</article>

      
    



<div class="list-pagination w-100">
  <ul class="paginate list pl0 tc dt w-100 ma0">
    <li class="dtc w-25 tl v-mid">
    
      <a href="/publications/" class="previous pa2 db fw6" aria-label="Previous">&larr; Newer</a>
    
    </li>
    <li class="dtc w-50 v-mid">
      <span class="page_number fw6 f7 ttu tracked">2 of 3</span>
    </li>
    <li class="dtc w-25 tr v-mid">
    
      <a href="/publications/page/3/" class="next pa2 db fw6" aria-label="Next">Older &rarr;</a>
    
    </li>
  </ul>
</div>


  </section>
</main>
<aside class="page-sidebar" role="complementary">
                         
 


                       
 











  <img src="/publications/sidebar-listing.jpg" class="db ma0">


<div class="blog-info ph4 pt4 pb4 pb0-l">
  

  <h1 class="f3">Academic Publications (by date)</h1>
  <p class="f6 lh-copy measure">Here&rsquo;s a list of my publications along with links to the publications and PDFs.</p>
  <p class="f7 measure lh-copy i mh0-l">Written by Violet Brown</p>


  
  <div class="dib bt bw1 b--black-10">
    
      <small class="db f7 pt3"><a href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" class="dib fw7 ttu">Google Scholar</a></small>
    
    
    
    
  </div>


</div>


</aside>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 Violet A. Brown
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/profile/violetsarebrown.bsky.social" title="cloud-sun" target="_blank" rel="me noopener">
      <i class="fas fa-cloud-sun fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/violetsarebrown?lang=en" title="twitter" target="_blank" rel="me noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://osf.io/fzqc8/" title="osf" target="_blank" rel="me noopener">
      <i class="ai ai-osf fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/violetbrownpsych/apero-website" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0001-5310-6499" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="ai ai-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:violet@violetabrown.com" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
