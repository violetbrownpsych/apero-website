<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>audiovisual integration on Violet Brown</title>
    <link>https://www.violetabrown.com/tags/audiovisual-integration/</link>
    <description>Recent content in audiovisual integration on Violet Brown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 15 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.violetabrown.com/tags/audiovisual-integration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Measuring the Dual-Task Costs of Audiovisual Speech Processing Across Levels of Background Noise </title>
      <link>https://www.violetabrown.com/publications/brown_2025/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/brown_2025/</guid>
      <description>Successful communication requires that listeners not only identify speech, but do so while maintaining performance on other tasks, like remembering what a conversational partner said or paying attention while driving. This set of four experiments systematically evaluated how audiovisual speech—which reliably improves speech intelligibility—affects dual-task costs during speech perception (i.e., one facet of listening effort). Results indicated that audiovisual speech reduces dual-task costs in difficult listening conditions (those in which visual cues substantially benefit intelligibility), but may actually increase costs in easy conditions—a pattern of results that was internally replicated multiple times.</description>
    </item>
    <item>
      <title>Speech and Non-Speech Measures of Audiovisual Integration are not Correlated</title>
      <link>https://www.violetabrown.com/publications/wilbiks_etal_2022/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.violetabrown.com/publications/wilbiks_etal_2022/</guid>
      <description>Many natural events generate both visual and auditory signals, and humans are remarkably adept at integrating information from those sources. However, individuals appear to differ markedly in their ability or propensity to combine what they hear with what they see. Individual differences in audiovisual integration have been established using a range of materials, including speech stimuli (seeing and hearing a talker) and simpler audiovisual stimuli (seeing flashes of light combined with tones).</description>
    </item>
  </channel>
</rss>
