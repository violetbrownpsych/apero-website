<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.120.4">
<title>audiovisual speech | Violet Brown</title>


<meta property="twitter:site" content="@violetsarebrown">
<meta property="twitter:creator" content="@violetsarebrown">







  
    
  
<meta name="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">


<meta property="og:site_name" content="Violet Brown">
<meta property="og:title" content="audiovisual speech | Violet Brown">
<meta property="og:description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College." />
<meta property="og:type" content="website" />
<meta property="og:url" content="/tags/audiovisual-speech/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="/img/VB_logo.PNG" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="/img/VB_logo.PNG" >
    
  <meta itemprop="name" content="audiovisual speech">
<meta itemprop="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/VB_logo.PNG" type="image/x-icon">
  <link rel="icon" href="/img/VB_logo.PNG" type="image/x-icon">
  
    <link rel="alternate" type="application/rss+xml" href="/tags/audiovisual-speech/index.xml" title="Violet Brown" />
  
  
  <link rel="stylesheet" href="/style.main.min.fc600ce742546fe629720c5a8ad0a1143c5cccce1be48e78180e574fed054b26.css" integrity="sha256-/GAM50JUb&#43;YpcgxaitChFDxczM4b5I54GA5XT&#43;0FSyY=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.af225ba7ae73e399b95c256b2414b3c615f1bbd4b09e693b64428703b6435d0d.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="/" title="Home">
      <img src="/img/VB_logo.PNG" class="dib db-l h2 w-auto" alt="Violet Brown">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publications/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/pdfs/brown_cv.pdf" title="CV">CV</a>
      
      
    </div>
  </nav>
</header>


<main class="page-main pa4" role="main">
  <section class="blog-intro mw7 center mb5">
    <h1 class="f2 f1-ns fw4 lh-solid tc center i">audiovisual speech</h1>
    <p class="f6 f5-ns measure lh-copy tc center"></p>
  </section>
  <section class="blog-content mw7 center">
  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/sewell_etal_2023/" class="db">The Effects of Temporal Cues, Point-Light Displays, and Faces on Speech Identification and Listening Effort</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Among the most robust findings in speech research is that the presence of a talking face improves the intelligibility of spoken language. Talking faces supplement the auditory signal by providing fine phonetic cues based on the placement of the articulators, as well as temporal cues to when speech is occurring. In this study, we varied the amount of information contained in the visual signal, ranging from temporal information alone to a natural talking face. Participants were presented with spoken sentences in energetic or informational masking in four different visual conditions: audio-only, a modulating circle providing temporal cues to salient features of the speech, a digitally rendered point-light display showing lip movement, and a natural talking face. We assessed both sentence identification accuracy and self-reported listening effort. Audiovisual benefit for intelligibility was observed for the natural face in both informational and energetic masking, but the digitally rendered point-light display only provided benefit in energetic masking. Intelligibility for speech accompanied by the modulating circle did not differ from the audio-only conditions in either masker type. Thus, the temporal cues used here were insufficient to improve speech intelligibility in noise, but some types of digital point-light displays may contain enough phonetic detail to produce modest improvements in speech identification in noise.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/mclaughlin_etal_2022/" class="db">Revisiting the Relationship Between Implicit Racial Bias and Audiovisual Benefit for Nonnative-Accented Speech</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Speech intelligibility is improved when the listener can see the talker in addition to hearing their voice. Notably, though,  previous work has suggested that this “audiovisual benefit” for nonnative (i.e., foreign-accented) speech is smaller than the  benefit for native speech, an effect that may be partially accounted for by listeners’ implicit racial biases (Yi et al., 2013, The  Journal of the Acoustical Society of America, 134[5], EL387–EL393.). In the present study, we sought to replicate these find- ings in a significantly larger sample of online participants. In a direct replication of Yi et al. (Experiment 1), we found that  audiovisual benefit was indeed smaller for nonnative-accented relative to native-accented speech. However, our results did  not support the conclusion that implicit racial biases, as measured with two types of implicit association tasks, were related  to these differences in audiovisual benefit for native and nonnative speech. In a second experiment, we addressed a potential  confound in the experimental design; to ensure that the difference in audiovisual benefit was caused by a difference in accent  rather than a difference in overall intelligibility, we reversed the overall difficulty of each accent condition by presenting them  at different signal-to-noise ratios. Even when native speech was presented at a much more difficult intelligibility level than  nonnative speech, audiovisual benefit for nonnative speech remained poorer. In light of these findings, we discuss alternative  explanations of reduced audiovisual benefit for nonnative speech, as well as methodological considerations for future work  examining the intersection of social, cognitive, and linguistic processes.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/strand_brown_barbour_2020/" class="db">Talking Points: A Modulating Circle Increases Listening Effort Without Improving Speech Recognition in Young Adults</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Speech recognition is improved when the acoustic input is accompanied by visual cues provided by a talking face (Erber in Journal of Speech and Hearing Research, 12(2), 423–425, 1969; Sumby &amp; Pollack in The Journal of the Acoustical Society of America, 26(2), 212–215, 1954). One way that the visual signal facilitates speech recognition is by providing the listener with information about fine phonetic detail that complements information from the auditory signal. However, given that degraded face stimuli can still improve speech recognition accuracy (Munhall, Kroos, Jozan, &amp; Vatikiotis-Bateson in Perception &amp; Psychophysics, 66(4), 574–583, 2004), and static or moving shapes can improve speech detection accuracy (Bernstein, Auer, &amp; Takayanagi in Speech Communication, 44(1–4), 5–18, 2004), aspects of the visual signal other than fine phonetic detail may also contribute to the perception of speech. In two experiments, we show that a modulating circle providing information about the onset, offset, and acoustic amplitude envelope of the speech does not improve recognition of spoken sentences (Experiment 1) or words (Experiment 2). Further, contrary to our hypothesis, the modulating circle increased listening effort despite subjective reports that it made the word recognition task seem easier to complete (Experiment 2). These results suggest that audiovisual speech processing, even when the visual stimulus only conveys temporal information about the acoustic signal, may be a cognitively demanding process.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_strand_2019/" class="db">About Face: Seeing the Talker Improves Spoken  Word Recognition but Increases Listening Effort</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">It is widely accepted that seeing a talker improves a listener’s ability to understand what a talker is saying in background noise (e.g., Erber, 1969; Sumby &amp; Pollack, 1954). The literature is mixed, however, regarding the influence of the visual modality on the listening effort required to recognize speech (e.g., Fraser, Gagné, Alepins, &amp; Dubois, 2010; Sommers &amp; Phelps, 2016). Here, we present data showing that even when the visual modality robustly benefits recognition, processing audiovisual speech can still result in greater cognitive load than processing speech in the auditory modality alone. We show using a dual-task paradigm that the costs associated with audiovisual speech processing are more pronounced in easy listening conditions, in which speech can be recognized at high rates in the auditory modality alone—indeed, effort did not differ between audiovisual and audio-only conditions when the background noise was presented at a more difficult level. Further, we show that though these effects replicate with different stimuli and participants, they do not emerge when effort is assessed with a recall paradigm rather than a dual-task paradigm. Together, these results suggest that the widely cited audiovisual recognition benefit may come at a cost under more favorable listening conditions, and add to the growing body of research suggesting that various measures of effort may not be tapping into the same underlying construct (Strand et al., 2018).</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_strand_2019_app/" class="db">&ldquo;Paying&rdquo; Attention to Audiovisual Speech: Do Incongruent Stimuli Incur Greater Costs?</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The McGurk effect is a multisensory phenomenon in which discrepant auditory and visual speech signals typically result in an illusory percept. McGurk stimuli are often used in studies assessing the attentional requirements of audiovisual integration, but no study has directly compared the costs associated with integrating congruent versus incongruent audiovisual speech. Some evidence suggests that the McGurk effect may not be representative of naturalistic audiovisual speech processing – susceptibility to the McGurk effect is not associated with the ability to derive benefit from the addition of the visual signal, and distinct cortical regions are recruited when processing congruent versus incongruent speech. In two experiments, one using response times to identify congruent and incongruent syllables and one using a dual-task paradigm, we assessed whether congruent and incongruent audiovisual speech incur different attentional costs. We demonstrated that response times to both the speech task (Experiment 1) and a secondary vibrotactile task (Experiment 2) were indistinguishable for congruent compared to incongruent syllables, but McGurk fusions were responded to more quickly than McGurk non-fusions. These results suggest that despite documented differences in how congruent and incongruent stimuli are processed, they do not appear to differ in terms of processing time or effort, at least in the open-set task speech task used here. However, responses that result in McGurk fusions are processed more quickly than those that result in non-fusions, though attentional cost is comparable for the two response types.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 Violet A. Brown
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/profile/violetsarebrown.bsky.social" title="cloud-sun" target="_blank" rel="me noopener">
      <i class="fas fa-cloud-sun fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/violetsarebrown?lang=en" title="twitter" target="_blank" rel="me noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://osf.io/fzqc8/" title="osf" target="_blank" rel="me noopener">
      <i class="ai ai-osf fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/violetbrownpsych/apero-website" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0001-5310-6499" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="ai ai-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:violetbrownpsych@gmail.com" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
