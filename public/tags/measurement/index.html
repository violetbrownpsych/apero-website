<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.120.4">
<title>measurement | Violet Brown</title>


<meta property="twitter:site" content="@violetsarebrown">
<meta property="twitter:creator" content="@violetsarebrown">







  
    
  
<meta name="description" content="Visiting Assistant Professor of Psychology at Carleton College.">


<meta property="og:site_name" content="Violet Brown">
<meta property="og:title" content="measurement | Violet Brown">
<meta property="og:description" content="Visiting Assistant Professor of Psychology at Carleton College." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.violetabrown.com/tags/measurement/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://www.violetabrown.com/img/VB_logo.PNG" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://www.violetabrown.com/img/VB_logo.PNG" >
    
  <meta itemprop="name" content="measurement">
<meta itemprop="description" content="Visiting Assistant Professor of Psychology at Carleton College.">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/VB_logo.PNG" type="image/x-icon">
  <link rel="icon" href="/img/VB_logo.PNG" type="image/x-icon">
  
    <link rel="alternate" type="application/rss+xml" href="https://www.violetabrown.com/tags/measurement/index.xml" title="Violet Brown" />
  
  
  <link rel="stylesheet" href="/style.main.min.fc600ce742546fe629720c5a8ad0a1143c5cccce1be48e78180e574fed054b26.css" integrity="sha256-/GAM50JUb&#43;YpcgxaitChFDxczM4b5I54GA5XT&#43;0FSyY=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.246e5a66bb6a18a599cc04627eb601304290680d3fc34f912389a3a5d8123494.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://www.violetabrown.com" title="Home">
      <img src="/img/VB_logo.PNG" class="dib db-l h2 w-auto" alt="Violet Brown">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publications/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/resources_tutorials" title="Resources and Tutorials">Resources and Tutorials</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="http://violetabrown.com/pdfs/brown_cv.pdf" title="CV">CV</a>
      
      
    </div>
  </nav>
</header>


<main class="page-main pa4" role="main">
  <section class="blog-intro mw7 center mb5">
    <h1 class="f2 f1-ns fw4 lh-solid tc center i">measurement</h1>
    <p class="f6 f5-ns measure lh-copy tc center"></p>
  </section>
  <section class="blog-content mw7 center">
  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_2025/" class="db">Measuring the Dual-Task Costs of Audiovisual Speech Processing Across Levels of Background Noise</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Successful communication requires that listeners not only identify speech, but do so while maintaining performance on other tasks, like remembering what a conversational partner said or paying attention while driving. This set of four experiments systematically evaluated how audiovisual speech—which reliably improves speech intelligibility—affects dual-task costs during speech perception (i.e., one facet of listening effort). Results indicated that audiovisual speech reduces dual-task costs in difficult listening conditions (those in which visual cues substantially benefit intelligibility), but may actually increase costs in easy conditions—a pattern of results that was internally replicated multiple times. This study also presents a novel dual-task paradigm specifically designed to facilitate conducting dual-task research remotely. Given the novelty of the task, this study includes psychometric experiments that establish positive and negative control, assess convergent validity, measure task sensitivity relative to a commonly-used dual-task paradigm, and generate performance curves across a range of listening conditions. Thus, in addition to evaluating the effects of audiovisual speech across a wide range of background noise levels, this study enables other researchers to address theoretical questions related to the cognitive mechanisms supporting speech processing beyond the specific issues addressed here and without being limited to in-person research.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/wilbiks_etal_2022/" class="db">Speech and Non-Speech Measures of Audiovisual Integration are not Correlated</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Many natural events generate both visual and auditory signals, and humans are remarkably adept at integrating information from those sources. However, individuals appear to differ markedly in their ability or propensity to combine what they hear with what they see. Individual differences in audiovisual integration have been established using a range of materials, including speech stimuli (seeing and hearing a talker) and simpler audiovisual stimuli (seeing flashes of light combined with tones). Although there are multiple tasks in the literature that are referred to as “measures of audiovisual integration,” the tasks themselves differ widely with respect to both the type of stimuli used (speech versus non-speech) and the nature of the tasks themselves (e.g., some tasks use conflicting auditory and visual stimuli whereas others use congruent stimuli). It is not clear whether these varied tasks are actually measuring the same underlying construct: audiovisual integration. This study tested the relationships among four commonly-used measures of audiovisual integration, two of which use speech stimuli (susceptibility to the McGurk effect and a measure of audiovisual benefit), and two of which use non-speech stimuli (the sound-induced flash illusion and audiovisual integration capacity). We replicated previous work showing large individual differences in each measure but found no significant correlations among any of the measures. These results suggest that tasks that are commonly referred to as measures of audiovisual integration may be tapping into different parts of the same process or different constructs entirely.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/resources_tutorials/strand_etal_2021/" class="db">Understanding Speech Amid the Jingle and Jangle: Recommendations for Improving Measurement Practices in Listening Effort Research</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The latent constructs psychologists study are typically not directly accessible, so researchers must design measurement instruments that are intended to provide insights about those constructs. Construct validation—assessing whether instruments measure what they intend to—is therefore critical for ensuring that the conclusions we draw actually reflect the intended phenomena. Insufficient construct validation can lead to the jingle fallacy—falsely assuming two instruments measure the same construct because the instruments share a name—and the jangle fallacy—falsely assuming two instruments measure different constructs because the instruments have different names. In this paper, we examine construct validation practices in research on listening effort and identify patterns that strongly suggest the presence of jingle and jangle in the literature. We argue that the lack of construct validation for listening effort measures has led to inconsistent findings and hindered our understanding of the construct. We also provide specific recommendations for improving construct validation of listening effort instruments, drawing on the framework laid out in a recent paper on improving measurement practices. Although this paper addresses listening effort, the issues raised and recommendations presented are widely applicable to tasks used in research on auditory perception and cognitive psychology.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/strand_etal_2021/" class="db">Understanding Speech Amid the Jingle and Jangle: Recommendations for Improving Measurement Practices in Listening Effort Research</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The latent constructs psychologists study are typically not directly accessible, so researchers must design measurement instruments that are intended to provide insights about those constructs. Construct validation—assessing whether instruments measure what they intend to—is therefore critical for ensuring that the conclusions we draw actually reflect the intended phenomena. Insufficient construct validation can lead to the jingle fallacy—falsely assuming two instruments measure the same construct because the instruments share a name—and the jangle fallacy—falsely assuming two instruments measure different constructs because the instruments have different names. In this paper, we examine construct validation practices in research on listening effort and identify patterns that strongly suggest the presence of jingle and jangle in the literature. We argue that the lack of construct validation for listening effort measures has led to inconsistent findings and hindered our understanding of the construct. We also provide specific recommendations for improving construct validation of listening effort instruments, drawing on the framework laid out in a recent paper on improving measurement practices. Although this paper addresses listening effort, the issues raised and recommendations presented are widely applicable to tasks used in research on auditory perception and cognitive psychology.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_etal_2020/" class="db">Rapid Adaptation to Fully Intelligible Nonnative-Accented Speech Reduces Listening Effort</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">In noisy settings or when listening to an unfamiliar talker or accent, it can be difficult to understand spoken language. This difficulty typically results in reductions in speech intelligibility, but may also increase the effort necessary to process the speech even when intelligibility is unaffected. In this study, we used a dual-task paradigm and pupillometry to assess the cognitive costs associated with processing fully intelligible accented speech, predicting that rapid perceptual adaptation to an accent would result in decreased listening effort over time. The behavioural and physiological paradigms provided converging evidence that listeners expend greater effort when processing nonnative- relative to native-accented speech, and both experiments also revealed an overall reduction in listening effort over the course of the experiment. Only the pupillometry experiment, however, revealed greater adaptation to nonnative- relative to native-accented speech. An exploratory analysis of the dual-task data that attempted to minimise practice effects revealed weak evidence for greater adaptation to the nonnative accent. These results suggest that even when speech is fully intelligible, resolving deviations between the acoustic input and stored lexical representations incurs a processing cost, and adaptation may attenuate this cost.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_strand_2019/" class="db">About Face: Seeing the Talker Improves Spoken  Word Recognition but Increases Listening Effort</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">It is widely accepted that seeing a talker improves a listener’s ability to understand what a talker is saying in background noise (e.g., Erber, 1969; Sumby &amp; Pollack, 1954). The literature is mixed, however, regarding the influence of the visual modality on the listening effort required to recognize speech (e.g., Fraser, Gagné, Alepins, &amp; Dubois, 2010; Sommers &amp; Phelps, 2016). Here, we present data showing that even when the visual modality robustly benefits recognition, processing audiovisual speech can still result in greater cognitive load than processing speech in the auditory modality alone. We show using a dual-task paradigm that the costs associated with audiovisual speech processing are more pronounced in easy listening conditions, in which speech can be recognized at high rates in the auditory modality alone—indeed, effort did not differ between audiovisual and audio-only conditions when the background noise was presented at a more difficult level. Further, we show that though these effects replicate with different stimuli and participants, they do not emerge when effort is assessed with a recall paradigm rather than a dual-task paradigm. Together, these results suggest that the widely cited audiovisual recognition benefit may come at a cost under more favorable listening conditions, and add to the growing body of research suggesting that various measures of effort may not be tapping into the same underlying construct (Strand et al., 2018).</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_strand_2019_app/" class="db">&ldquo;Paying&rdquo; Attention to Audiovisual Speech: Do Incongruent Stimuli Incur Greater Costs?</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The McGurk effect is a multisensory phenomenon in which discrepant auditory and visual speech signals typically result in an illusory percept. McGurk stimuli are often used in studies assessing the attentional requirements of audiovisual integration, but no study has directly compared the costs associated with integrating congruent versus incongruent audiovisual speech. Some evidence suggests that the McGurk effect may not be representative of naturalistic audiovisual speech processing – susceptibility to the McGurk effect is not associated with the ability to derive benefit from the addition of the visual signal, and distinct cortical regions are recruited when processing congruent versus incongruent speech. In two experiments, one using response times to identify congruent and incongruent syllables and one using a dual-task paradigm, we assessed whether congruent and incongruent audiovisual speech incur different attentional costs. We demonstrated that response times to both the speech task (Experiment 1) and a secondary vibrotactile task (Experiment 2) were indistinguishable for congruent compared to incongruent syllables, but McGurk fusions were responded to more quickly than McGurk non-fusions. These results suggest that despite documented differences in how congruent and incongruent stimuli are processed, they do not appear to differ in terms of processing time or effort, at least in the open-set task speech task used here. However, responses that result in McGurk fusions are processed more quickly than those that result in non-fusions, though attentional cost is comparable for the two response types.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/strand_etal_2018/" class="db">Measuring Listening Effort: Convergent Validity, Sensitivity, and Links With Cognitive and Personality Measures</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Purpose: Listening effort (LE) describes the attentional or cognitive requirements for successful listening. Despite substantial theoretical and clinical interest in LE, inconsistent operationalization makes it difficult to make generalizations across studies. The aims of this large-scale validation study were to evaluate the convergent validity and sensitivity of commonly used measures of LE and assess how scores on those tasks relate to cognitive and personality variables.
Method: Young adults with normal hearing (N = 111) completed 7 tasks designed to measure LE, 5 tests of cognitive ability, and 2 personality measures.
Results: Scores on some behavioral LE tasks were moderately intercorrelated but were generally not correlated with subjective and physiological measures of LE, suggesting that these tasks may not be tapping into the same underlying construct. LE measures differed in their sensitivity to changes in signal-to-noise ratio and the extent to which they correlated with cognitive and personality variables.
Conclusions: Given that LE measures do not show consistent, strong intercorrelations and differ in their relationships with cognitive and personality predictors, these findings suggest caution in generalizing across studies that use different measures of LE. The results also indicate that people with greater cognitive ability appear to use their resources more efficiently, thereby diminishing the detrimental effects associated with increased background noise during language processing.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2026 Violet A. Brown
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/profile/violetsarebrown.bsky.social" title="cloud-sun" target="_blank" rel="me noopener">
      <i class="fas fa-cloud-sun fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/violetsarebrown?lang=en" title="twitter" target="_blank" rel="me noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://osf.io/fzqc8/" title="osf" target="_blank" rel="me noopener">
      <i class="ai ai-osf fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/violetbrownpsych/apero-website" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0001-5310-6499" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="ai ai-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:violet@violetabrown.com" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
