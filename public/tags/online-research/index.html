<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.120.4">
<title>online research | Violet Brown</title>


<meta property="twitter:site" content="@violetsarebrown">
<meta property="twitter:creator" content="@violetsarebrown">







  
    
  
<meta name="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">


<meta property="og:site_name" content="Violet Brown">
<meta property="og:title" content="online research | Violet Brown">
<meta property="og:description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.violetabrown.com/tags/online-research/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://www.violetabrown.com/img/VB_logo.PNG" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://www.violetabrown.com/img/VB_logo.PNG" >
    
  <meta itemprop="name" content="online research">
<meta itemprop="description" content="Postdoctoral Fellow in Dr. Julia Strand&#39;s Perception Lab at Carleton College.">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/VB_logo.PNG" type="image/x-icon">
  <link rel="icon" href="/img/VB_logo.PNG" type="image/x-icon">
  
    <link rel="alternate" type="application/rss+xml" href="https://www.violetabrown.com/tags/online-research/index.xml" title="Violet Brown" />
  
  
  <link rel="stylesheet" href="/style.main.min.fc600ce742546fe629720c5a8ad0a1143c5cccce1be48e78180e574fed054b26.css" integrity="sha256-/GAM50JUb&#43;YpcgxaitChFDxczM4b5I54GA5XT&#43;0FSyY=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.246e5a66bb6a18a599cc04627eb601304290680d3fc34f912389a3a5d8123494.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://www.violetabrown.com" title="Home">
      <img src="/img/VB_logo.PNG" class="dib db-l h2 w-auto" alt="Violet Brown">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publications/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/pdfs/brown_cv.pdf" title="CV">CV</a>
      
      
    </div>
  </nav>
</header>


<main class="page-main pa4" role="main">
  <section class="blog-intro mw7 center mb5">
    <h1 class="f2 f1-ns fw4 lh-solid tc center i">online research</h1>
    <p class="f6 f5-ns measure lh-copy tc center"></p>
  </section>
  <section class="blog-content mw7 center">
  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/sewell_etal_2023/" class="db">The Effects of Temporal Cues, Point-Light Displays, and Faces on Speech Identification and Listening Effort</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Among the most robust findings in speech research is that the presence of a talking face improves the intelligibility of spoken language. Talking faces supplement the auditory signal by providing fine phonetic cues based on the placement of the articulators, as well as temporal cues to when speech is occurring. In this study, we varied the amount of information contained in the visual signal, ranging from temporal information alone to a natural talking face. Participants were presented with spoken sentences in energetic or informational masking in four different visual conditions: audio-only, a modulating circle providing temporal cues to salient features of the speech, a digitally rendered point-light display showing lip movement, and a natural talking face. We assessed both sentence identification accuracy and self-reported listening effort. Audiovisual benefit for intelligibility was observed for the natural face in both informational and energetic masking, but the digitally rendered point-light display only provided benefit in energetic masking. Intelligibility for speech accompanied by the modulating circle did not differ from the audio-only conditions in either masker type. Thus, the temporal cues used here were insufficient to improve speech intelligibility in noise, but some types of digital point-light displays may contain enough phonetic detail to produce modest improvements in speech identification in noise.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_etal_2022/" class="db">Revisiting the Target-Masker Linguistic Similarity Hypothesis</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The linguistic similarity hypothesis states that it is more difficult to segregate target and masker speech when they are linguistically similar. For example, recognition of English target speech should be more impaired by the presence of Dutch masking speech than Mandarin masking speech because Dutch and English are more linguistically similar than Mandarin and English. Across four experiments, English target speech was consistently recognized more poorly when presented in English masking speech than in silence, speech-shaped noise, or an unintelligible masker (i.e., Dutch or Mandarin). However, we found no evidence for graded masking effects—Dutch did not impair performance more than Mandarin in any experiment, despite 650 participants being tested. This general pattern was consistent when using both a cross-modal paradigm (in which target speech was lipread and maskers were presented aurally; Experiments 1a and 1b) and an auditory-only paradigm (in which both the targets and maskers were presented aurally; Experiments 2a and 2b). These findings suggest that the linguistic similarity hypothesis should be refined to reflect the existing evidence: There is greater release from masking when the masker language differs from the target speech than when it is the same as the target speech. However, evidence that unintelligible maskers impair speech identification to a greater extent when they are more linguistically similar to the target language remains elusive.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/mclaughlin_etal_2022/" class="db">Revisiting the Relationship Between Implicit Racial Bias and Audiovisual Benefit for Nonnative-Accented Speech</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Speech intelligibility is improved when the listener can see the talker in addition to hearing their voice. Notably, though,  previous work has suggested that this “audiovisual benefit” for nonnative (i.e., foreign-accented) speech is smaller than the  benefit for native speech, an effect that may be partially accounted for by listeners’ implicit racial biases (Yi et al., 2013, The  Journal of the Acoustical Society of America, 134[5], EL387–EL393.). In the present study, we sought to replicate these find- ings in a significantly larger sample of online participants. In a direct replication of Yi et al. (Experiment 1), we found that  audiovisual benefit was indeed smaller for nonnative-accented relative to native-accented speech. However, our results did  not support the conclusion that implicit racial biases, as measured with two types of implicit association tasks, were related  to these differences in audiovisual benefit for native and nonnative speech. In a second experiment, we addressed a potential  confound in the experimental design; to ensure that the difference in audiovisual benefit was caused by a difference in accent  rather than a difference in overall intelligibility, we reversed the overall difficulty of each accent condition by presenting them  at different signal-to-noise ratios. Even when native speech was presented at a much more difficult intelligibility level than  nonnative speech, audiovisual benefit for nonnative speech remained poorer. In light of these findings, we discuss alternative  explanations of reduced audiovisual benefit for nonnative speech, as well as methodological considerations for future work  examining the intersection of social, cognitive, and linguistic processes.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_vanengen_peelle_2021/" class="db">Face Mask Type Affects Audiovisual Speech Intelligibility and Subjective Listening Effort in Young and Older Adults</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">Identifying speech requires that listeners make rapid use of fine-grained acoustic cues—a process that is facilitated by being able to see the talker’s face. Face masks present a challenge to this process because they can both alter acoustic information and conceal the talker’s mouth. Here, we investigated the degree to which different types of face masks and noise levels affect speech intelligibility and subjective listening effort for young (N = 180) and older (N = 180) adult listeners. We found that in quiet, mask type had little influence on speech intelligibility relative to speech produced without a mask for both young and older adults. However, with the addition of moderate (− 5 dB SNR) and high (− 9 dB SNR) levels of background noise, intelligibility dropped substantially for all types of face masks in both age groups. Across noise levels, transparent face masks and cloth face masks with filters impaired performance the most, and surgical face masks had the smallest influence on intelligibility. Participants also rated speech produced with a face mask as more effortful than unmasked speech, particularly in background noise. Although young and older adults were similarly affected by face masks and noise in terms of intelligibility and subjective listening effort, older adults showed poorer intelligibility overall and rated the speech as more effortful to process relative to young adults. This research will help individuals make more informed decisions about which types of masks to wear in various communicative settings.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/guang_etal_2021/" class="db">Recall of Speech is Impaired by Subsequent Masking Noise: A Replication of Rabbitt (1968) Experiment 2</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The presence of masking noise can impair speech intelligibility and increase the cognitive resources necessary to understand speech. The first study to demonstrate the negative cognitive consequences of noisy speech—published by Rabbitt in 1968—found that participants had poorer recall for aurally presented digits early in a list when later digits were presented in noise relative to quiet. However, despite being cited nearly 500 times and providing the foundation for a wealth of subsequent research on the topic, the original study has never been directly replicated. Here we report a replication attempt of that study with a large online sample and tested the robustness of the results to a variety of scoring and analytical techniques. We replicated the key finding that listening to speech in noise impairs recall for items that came earlier in the list. The results were consistent when we used the original analytical technique (an ANOVA) and a more powerful analytical technique (generalized linear mixed effects models) that was not available when the original paper was published. These findings support the claim that effortful listening can interfere with encoding or rehearsal of previously presented information.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
    
                               
     
     
<article class="mv4 mv5-l bb">

  
  
  <div class="measure-wide center mb4 mb5-l">
    <header>
      <h1 class="f3 mt0 mb2 lh-title underline fw4"><a href="/publications/brown_etal_2018/" class="db">What Accounts for Individual Differences in Susceptibility to the McGurk Effect?</a></h1>
      
    </header>
      
      <p class="lh-copy mb4">The McGurk effect is a classic audiovisual speech illusion in which discrepant auditory and visual syllables can lead to a fused percept (e.g., an auditory /bɑ/ paired with a visual /gɑ/ often leads to the perception of /dɑ/). The McGurk effect is robust and easily replicated in pooled group data, but there is tremendous variability in the extent to which individual participants are susceptible to it. In some studies, the rate at which individuals report fusion responses ranges from 0% to 100%. Despite its widespread use in the audiovisual speech perception literature, the roots of the wide variability in McGurk susceptibility are largely unknown. This study evaluated whether several perceptual and cognitive traits are related to McGurk susceptibility through correlational analyses and mixed effects modeling. We found that an individual’s susceptibility to the McGurk effect was related to their ability to extract place of articulation information from the visual signal (i.e., a more fine-grained anal- ysis of lipreading ability), but not to scores on tasks measuring attentional control, processing speed, working memory capacity, or auditory perceptual gradiency. These results provide support for the claim that a small amount of the variability in susceptibility to the McGurk effect is attributable to lipreading skill. In contrast, cognitive and perceptual abilities that are commonly used predictors in individual differences studies do not appear to underlie susceptibility to the McGurk effect.</p>
    <footer>
      
      
      
    </footer>
  </div>
  
</article>

  
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 Violet A. Brown
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/profile/violetsarebrown.bsky.social" title="cloud-sun" target="_blank" rel="me noopener">
      <i class="fas fa-cloud-sun fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/violetsarebrown?lang=en" title="twitter" target="_blank" rel="me noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://osf.io/fzqc8/" title="osf" target="_blank" rel="me noopener">
      <i class="ai ai-osf fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/violetbrownpsych/apero-website" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0001-5310-6499" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=SrLYUXMAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="ai ai-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:violet@violetabrown.com" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
